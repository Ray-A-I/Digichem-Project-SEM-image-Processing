{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac92d197",
   "metadata": {},
   "source": [
    "Using GPU to train VAE model, with Pretrain-Finetune Models.\n",
    "Ensure consistent latent dimension is used in both pretrain/finetune\n",
    "Change Filename Accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # Add this BEFORE importing torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ff2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder with Dropout\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            #nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.flattened_size = 128 * 18 * 32\n",
    "\n",
    "        self.fc_mu = nn.Linear(self.flattened_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flattened_size, latent_dim)\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, self.flattened_size)\n",
    "\n",
    "        # Decoder with light Dropout (optional)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        return self.fc_mu(x), self.fc_logvar(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 128, 18, 32)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68089c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Dataset class with error handling\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = [f for f in os.listdir(root_dir) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((144,256)),  # Maintain (H, W) ordering\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        print(f\"Total images loaded: {len(self.image_files)}\")  # Print total images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path)\n",
    "        return self.transform(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de013ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_single_image(model, device, image_path):\n",
    "    # Load and preprocess single image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((144, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Generate reconstruction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recon= model(image_tensor)\n",
    "    \n",
    "    # Convert tensors to numpy arrays\n",
    "    original = image_tensor.cpu().numpy()[0][0]\n",
    "    reconstruction = torch.sigmoid(recon).cpu().numpy()[0][0]\n",
    "    \n",
    "    #original_rotated = np.rot90(original, k=1)        # Rotate 90° counterclockwise\n",
    "    #reconstruction_rotated = np.rot90(reconstruction, k=1)\n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    axes[0].imshow(original, cmap='gray')\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(reconstruction, cmap='gray')\n",
    "    axes[1].set_title('Reconstruction')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f09d7da-49c7-4b92-b2b5-fdca9cdf2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def vae_loss_func(recon, target, mu, logvar, avg_power):\n",
    "    # --- Reconstruction Loss (NMSE) ---\n",
    "    recon_flat = recon.view(recon.size(0), -1)\n",
    "    target_flat = target.view(target.size(0), -1)\n",
    "\n",
    "    squared_error = torch.sum((recon_flat - target_flat) ** 2, dim=1)\n",
    "    nmse = squared_error / (avg_power + 1e-8)\n",
    "    recon_loss = nmse.mean()\n",
    "\n",
    "    # --- KL Divergence ---\n",
    "    # KL(N(mu, sigma) || N(0, 1)) = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    kl_loss = kl_div.mean()\n",
    "    ##print(recon_loss)\n",
    "    ##print(kl_loss)\n",
    "    # --- Total VAE Loss ---\n",
    "    return recon_loss, kl_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3264d08-cc1d-4d4b-a497-3eb7469ff1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "def compute_avg_power(loader, device):\n",
    "    total_power = 0.0\n",
    "    total_samples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        data_flat = data.view(data.size(0), -1)\n",
    "        total_power += torch.sum(data_flat ** 2).item()\n",
    "        total_samples += data.size(0)\n",
    "    return total_power / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc4522-69db-44d6-a824-40799879e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import random\n",
    "import os\n",
    "\n",
    "def main_pretrain(n_sample, random_seed, beta, log_path, latent_dim):\n",
    "    #main_pretrain takes 5 arguments:\n",
    "    # n_sample: Number of samples to use for pretraining\n",
    "    # random_seed: Seed for reproducibility\n",
    "    # beta: Weight for KL divergence term\n",
    "    # log_path: Path to save the training log\n",
    "    # latent_dim: Dimensionality of the latent space\n",
    "    \n",
    "    batch_size = 256\n",
    "    epochs = 20\n",
    "    lr = 3e-4\n",
    "\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True) if os.path.dirname(log_path) else None\n",
    "\n",
    "    print('loaded samples: ' + str(n_sample))\n",
    "\n",
    "    # Replace path with pretrain dataset\n",
    "    full_dataset = ImageDataset(root_dir=r'Preprocessed Images/Museum Art/processed_from_reference')\n",
    "    full_len = len(full_dataset)\n",
    "\n",
    "    # Randomly sample n_sample indices with seed\n",
    "    random.seed(random_seed)\n",
    "    sampled_indices = random.sample(range(full_len), k=min(n_sample, full_len))\n",
    "    dataset_1 = Subset(full_dataset, sampled_indices)\n",
    "\n",
    "    # Train/Val split\n",
    "    len1 = len(dataset_1)\n",
    "    train_1, val_1 = random_split(dataset_1, [int(0.8 * len1), len1 - int(0.8 * len1)],generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "    train_loader = DataLoader(train_1, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_1, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    # Model setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = VAE(latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    # Compute normalization power\n",
    "    avg_power = compute_avg_power(train_loader, device)\n",
    "    print(f\"[Pretrain] Avg power: {avg_power:.6f}\")\n",
    "    avg_power = 1\n",
    "\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(f\"[Pretrain] Start training with {n_sample} samples...\\n\")\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                recon, mu, logvar = model(data)\n",
    "                recon_loss, kl_loss = vae_loss_func(recon, data, mu, logvar, avg_power)\n",
    "                loss = recon_loss+kl_loss*beta\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_loss = 0.0\n",
    "            val_recon_loss = 0.0\n",
    "            val_kl_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    data = data.to(device)\n",
    "                    recon, mu, logvar = model(data)\n",
    "                    recon_loss, kl_loss = vae_loss_func(recon, data, mu, logvar, avg_power)\n",
    "                    combined_loss = recon_loss + kl_loss * beta\n",
    "            \n",
    "                    # Accumulate losses multiplied by batch size\n",
    "                    batch_size_actual = data.size(0)\n",
    "                    val_loss += combined_loss.item() * batch_size_actual\n",
    "                    val_recon_loss += recon_loss.item() * batch_size_actual\n",
    "                    val_kl_loss += kl_loss.item() * batch_size_actual\n",
    "            \n",
    "            # Compute per‑sample averages\n",
    "            val_loss_avg = val_loss / len(val_loader.dataset)\n",
    "            val_recon_loss_avg = val_recon_loss / len(val_loader.dataset)\n",
    "            val_kl_loss_avg = val_kl_loss / len(val_loader.dataset)\n",
    "            \n",
    "            log_line = (\n",
    "                f\"[Pretrain] Epoch {epoch+1:03d} | \"\n",
    "                f\"Train Loss: {train_loss / len(train_loader.dataset):.6f} | \"\n",
    "                f\"Val Loss: {val_loss_avg:.6f} | \"\n",
    "                f\"Val Recon loss: {val_recon_loss_avg:.6f} | \"\n",
    "                f\"Val KL loss: {val_kl_loss_avg:.6f}\"\n",
    "            )\n",
    "            print(log_line)\n",
    "            f.write(log_line+ \"\\n\")\n",
    "\n",
    "    # Save the model\n",
    "    #torch.save(model.state_dict(), \"ae_pretrained_on_dataset1.pth\")\n",
    "    print(\"[Pretrain] Model saved.\")\n",
    "    return model, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabeb95a-db73-4c0c-9c7d-4764dc2e6a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded samples: 80000\n",
      "Total images loaded: 124204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trained_model, device = main_pretrain(80000, 42,10,'Enter Log Name Here',2048)\n",
    "torch.save(trained_model.state_dict(), r\"Trained Models/Example VAE Pretrained Model.pth\") #12:08\n",
    "# ===== after finishing one training run =====\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# ===== now you can start next training run =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_finetune(latent_dim,beta,log_file,pretrained_model):\n",
    "    #main_finetune takes 4 arguments:\n",
    "    # latent_dim: Dimensionality of the latent space\n",
    "    # beta: Weight for KL divergence term\n",
    "    # log_file: Path to save the training log\n",
    "    # pretrained_model: Path to the pretrained model weights\n",
    "    \n",
    "    rand_seed = 42\n",
    "\n",
    "    batch_size = 256\n",
    "    epochs = 10\n",
    "    lr = 5e-5  # smaller LR for fine-tuning\n",
    "    #beta = 7\n",
    "\n",
    "\n",
    "    log_dir = os.path.dirname(log_file)\n",
    "    if log_dir:\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(\"[Finetune] Training log\\n\")\n",
    "\n",
    "    #change paths to your datasets, dataset_2 containes unlabeled SEM images, dataset_3 contains labeled SEM images\n",
    "    dataset_2 = ImageDataset(root_dir=r'Preprocessed Images\\Unlabeled SEM\\processed_smaller_20K_normgrey')\n",
    "    dataset_3 = ImageDataset(root_dir=r'Preprocessed Images\\Labeled SEM\\ourimg_normgrey')\n",
    "\n",
    "    len2 = len(dataset_2)\n",
    "    len3 = len(dataset_3)\n",
    "\n",
    "    train_2, val_2 = random_split(dataset_2, [int(0.8 * len2), len2 - int(0.8 * len2)],\n",
    "                                  generator=torch.Generator().manual_seed(rand_seed))\n",
    "    train_3, val_3 = random_split(dataset_3, [int(0.8 * len3), len3 - int(0.8 * len3)],\n",
    "                                  generator=torch.Generator().manual_seed(rand_seed))\n",
    "\n",
    "    train_dataset = ConcatDataset([train_2, train_3])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader_2 = DataLoader(val_2, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    val_loader_3 = DataLoader(val_3, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    full_fixed_loader = DataLoader(dataset_3, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = VAE(latent_dim).to(device)\n",
    "    model.load_state_dict(torch.load(pretrained_model))\n",
    "    print(\"[Finetune] Loaded pretrained weights.\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"[Finetune] Loaded pretrained weights.\\n\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-6)\n",
    "    avg_power = compute_avg_power(train_loader, device)\n",
    "    print(f\"[Finetune] Avg power: {avg_power:.6f}\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"[Finetune] Avg power: {avg_power:.6f}\\n\")\n",
    "\n",
    "    avg_power = 1\n",
    "    print(\"[Finetune] Start training...\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"[Finetune] Start training...\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(data)\n",
    "            recon_loss, kl_loss = vae_loss_func(recon, data, mu, logvar, avg_power)\n",
    "            loss = recon_loss + beta * kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "    \n",
    "        model.eval()\n",
    "        val_loss_2 = val_recon_2 = val_kl_2 = 0\n",
    "        val_loss_3 = val_recon_3 = val_kl_3 = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader_2:\n",
    "                data = data.to(device)\n",
    "                recon, mu, logvar = model(data)\n",
    "                recon_loss, kl_loss = vae_loss_func(recon, data, mu, logvar, avg_power)\n",
    "                total_loss = recon_loss + beta * kl_loss\n",
    "                val_loss_2 += total_loss.item() * data.size(0)\n",
    "                val_recon_2 += recon_loss.item() * data.size(0)\n",
    "                val_kl_2 += kl_loss.item() * data.size(0)\n",
    "    \n",
    "            for data in val_loader_3:\n",
    "                data = data.to(device)\n",
    "                recon, mu, logvar = model(data)\n",
    "                recon_loss, kl_loss = vae_loss_func(recon, data, mu, logvar, avg_power)\n",
    "                total_loss = recon_loss + beta * kl_loss\n",
    "                val_loss_3 += total_loss.item() * data.size(0)\n",
    "                val_recon_3 += recon_loss.item() * data.size(0)\n",
    "                val_kl_3 += kl_loss.item() * data.size(0)\n",
    "    \n",
    "        # normalize\n",
    "        train_loss_epoch = train_loss / len(train_loader.dataset)\n",
    "        val_loss_2 /= len(val_loader_2.dataset)\n",
    "        val_recon_2 /= len(val_loader_2.dataset)\n",
    "        val_kl_2 /= len(val_loader_2.dataset)\n",
    "    \n",
    "        val_loss_3 /= len(val_loader_3.dataset)\n",
    "        val_recon_3 /= len(val_loader_3.dataset)\n",
    "        val_kl_3 /= len(val_loader_3.dataset)\n",
    "    \n",
    "        # modified print\n",
    "        log_msg = (\n",
    "            f\"[Finetune] Epoch {epoch+1:03d} | \"\n",
    "            f\"Train Loss: {train_loss_epoch:.6f} | \"\n",
    "            f\"Val Loss (Main): {val_loss_2:.6f} (Recon: {val_recon_2:.6f}, KL: {val_kl_2:.6f}) | \"\n",
    "            f\"Val Loss (Fixed): {val_loss_3:.6f} (Recon: {val_recon_3:.6f}, KL: {val_kl_3:.6f})\"\n",
    "        )\n",
    "        print(log_msg)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(log_msg + \"\\n\")\n",
    "\n",
    "    return model, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de266c08-0094-446f-acad-0e189b75d41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 20538\n",
      "Total images loaded: 346\n",
      "[Finetune] Loaded pretrained weights.\n",
      "[Finetune] Avg power: 12049.319601\n",
      "[Finetune] Start training...\n",
      "[Finetune] Epoch 001 | Train Loss: 666.370977 | Val Loss (Main): 529.574552 (Recon: 520.772923, KL: 0.044008) | Val Loss (Fixed): 886.033508 (Recon: 880.319824, KL: 0.028568)\n",
      "[Finetune] Epoch 002 | Train Loss: 525.293154 | Val Loss (Main): 521.003921 (Recon: 516.210764, KL: 0.023966) | Val Loss (Fixed): 884.882324 (Recon: 881.521423, KL: 0.016805)\n",
      "[Finetune] Epoch 003 | Train Loss: 522.314826 | Val Loss (Main): 520.076727 (Recon: 515.403953, KL: 0.023364) | Val Loss (Fixed): 887.045776 (Recon: 882.725464, KL: 0.021602)\n",
      "[Finetune] Epoch 004 | Train Loss: 521.465858 | Val Loss (Main): 519.136449 (Recon: 515.303678, KL: 0.019164) | Val Loss (Fixed): 882.489319 (Recon: 878.649841, KL: 0.019197)\n",
      "[Finetune] Epoch 005 | Train Loss: 521.059641 | Val Loss (Main): 518.356155 (Recon: 515.128258, KL: 0.016140) | Val Loss (Fixed): 883.207886 (Recon: 879.873108, KL: 0.016674)\n",
      "[Finetune] Epoch 006 | Train Loss: 520.802548 | Val Loss (Main): 517.554950 (Recon: 515.086429, KL: 0.012343) | Val Loss (Fixed): 881.098389 (Recon: 878.560364, KL: 0.012690)\n",
      "[Finetune] Epoch 007 | Train Loss: 520.673731 | Val Loss (Main): 516.958852 (Recon: 515.000112, KL: 0.009794) | Val Loss (Fixed): 880.423096 (Recon: 878.465088, KL: 0.009790)\n",
      "[Finetune] Epoch 008 | Train Loss: 520.576566 | Val Loss (Main): 516.885842 (Recon: 515.042540, KL: 0.009217) | Val Loss (Fixed): 879.608765 (Recon: 877.718079, KL: 0.009454)\n",
      "[Finetune] Epoch 009 | Train Loss: 520.482721 | Val Loss (Main): 516.302577 (Recon: 514.929497, KL: 0.006865) | Val Loss (Fixed): 882.285400 (Recon: 880.884460, KL: 0.007005)\n",
      "[Finetune] Epoch 010 | Train Loss: 520.419643 | Val Loss (Main): 516.240645 (Recon: 514.931036, KL: 0.006548) | Val Loss (Fixed): 879.081116 (Recon: 877.775146, KL: 0.006530)\n",
      "Total images loaded: 20538\n",
      "Total images loaded: 346\n",
      "[Finetune] Loaded pretrained weights.\n",
      "[Finetune] Avg power: 12049.319717\n",
      "[Finetune] Start training...\n",
      "[Finetune] Epoch 001 | Train Loss: 2186.012746 | Val Loss (Main): 567.604593 (Recon: 519.799280, KL: 0.023903) | Val Loss (Fixed): 933.588501 (Recon: 884.790100, KL: 0.024399)\n",
      "[Finetune] Epoch 002 | Train Loss: 542.260238 | Val Loss (Main): 544.795409 (Recon: 516.062815, KL: 0.014366) | Val Loss (Fixed): 906.457947 (Recon: 880.519653, KL: 0.012969)\n",
      "[Finetune] Epoch 003 | Train Loss: 528.592655 | Val Loss (Main): 532.051340 (Recon: 515.478454, KL: 0.008286) | Val Loss (Fixed): 892.316650 (Recon: 881.182800, KL: 0.005567)\n",
      "[Finetune] Epoch 004 | Train Loss: 523.870968 | Val Loss (Main): 528.373127 (Recon: 515.172120, KL: 0.006601) | Val Loss (Fixed): 891.598022 (Recon: 882.615662, KL: 0.004491)\n",
      "[Finetune] Epoch 005 | Train Loss: 522.714634 | Val Loss (Main): 526.410920 (Recon: 515.186333, KL: 0.005612) | Val Loss (Fixed): 887.951355 (Recon: 880.191956, KL: 0.003880)\n",
      "[Finetune] Epoch 006 | Train Loss: 522.243662 | Val Loss (Main): 525.129407 (Recon: 515.088782, KL: 0.005020) | Val Loss (Fixed): 885.735779 (Recon: 878.619873, KL: 0.003558)\n",
      "[Finetune] Epoch 007 | Train Loss: 521.967447 | Val Loss (Main): 523.412091 (Recon: 515.028133, KL: 0.004192) | Val Loss (Fixed): 885.423889 (Recon: 879.851807, KL: 0.002786)\n",
      "[Finetune] Epoch 008 | Train Loss: 521.730508 | Val Loss (Main): 523.474298 (Recon: 514.935299, KL: 0.004269) | Val Loss (Fixed): 887.074463 (Recon: 880.805542, KL: 0.003134)\n",
      "[Finetune] Epoch 009 | Train Loss: 521.575471 | Val Loss (Main): 523.459856 (Recon: 514.850909, KL: 0.004304) | Val Loss (Fixed): 888.970337 (Recon: 882.660156, KL: 0.003155)\n",
      "[Finetune] Epoch 010 | Train Loss: 521.359977 | Val Loss (Main): 521.366171 (Recon: 514.917045, KL: 0.003225) | Val Loss (Fixed): 881.549561 (Recon: 876.936890, KL: 0.002306)\n",
      "Total images loaded: 20538\n",
      "Total images loaded: 346\n",
      "[Finetune] Loaded pretrained weights.\n",
      "[Finetune] Avg power: 12049.319604\n",
      "[Finetune] Start training...\n",
      "[Finetune] Epoch 001 | Train Loss: 3812.530495 | Val Loss (Main): 645.844527 (Recon: 518.508826, KL: 0.063668) | Val Loss (Fixed): 973.738159 (Recon: 881.977722, KL: 0.045880)\n",
      "[Finetune] Epoch 002 | Train Loss: 550.870418 | Val Loss (Main): 536.926859 (Recon: 515.959627, KL: 0.010484) | Val Loss (Fixed): 892.309326 (Recon: 880.250000, KL: 0.006030)\n",
      "[Finetune] Epoch 003 | Train Loss: 525.559168 | Val Loss (Main): 527.758629 (Recon: 515.540100, KL: 0.006109) | Val Loss (Fixed): 884.154480 (Recon: 878.283813, KL: 0.002935)\n",
      "[Finetune] Epoch 004 | Train Loss: 522.726569 | Val Loss (Main): 525.216334 (Recon: 515.203723, KL: 0.005006) | Val Loss (Fixed): 888.811035 (Recon: 882.361511, KL: 0.003225)\n",
      "[Finetune] Epoch 005 | Train Loss: 522.002444 | Val Loss (Main): 524.022577 (Recon: 515.114497, KL: 0.004454) | Val Loss (Fixed): 885.383057 (Recon: 879.144287, KL: 0.003119)\n",
      "[Finetune] Epoch 006 | Train Loss: 521.664316 | Val Loss (Main): 523.462583 (Recon: 514.994803, KL: 0.004234) | Val Loss (Fixed): 886.467285 (Recon: 880.352234, KL: 0.003058)\n",
      "[Finetune] Epoch 007 | Train Loss: 521.415816 | Val Loss (Main): 522.918120 (Recon: 515.146058, KL: 0.003886) | Val Loss (Fixed): 884.226135 (Recon: 878.587219, KL: 0.002819)\n",
      "[Finetune] Epoch 008 | Train Loss: 521.272459 | Val Loss (Main): 522.396661 (Recon: 514.893533, KL: 0.003752) | Val Loss (Fixed): 888.288147 (Recon: 882.829834, KL: 0.002729)\n",
      "[Finetune] Epoch 009 | Train Loss: 521.109522 | Val Loss (Main): 522.059416 (Recon: 515.003890, KL: 0.003528) | Val Loss (Fixed): 882.507385 (Recon: 877.319946, KL: 0.002594)\n",
      "[Finetune] Epoch 010 | Train Loss: 521.031669 | Val Loss (Main): 521.546164 (Recon: 514.987454, KL: 0.003279) | Val Loss (Fixed): 882.134155 (Recon: 877.483948, KL: 0.002325)\n",
      "Total images loaded: 20538\n",
      "Total images loaded: 346\n",
      "[Finetune] Loaded pretrained weights.\n",
      "[Finetune] Avg power: 12049.319679\n",
      "[Finetune] Start training...\n",
      "[Finetune] Epoch 001 | Train Loss: 4391.671857 | Val Loss (Main): 655.776026 (Recon: 519.687919, KL: 0.068044) | Val Loss (Fixed): 945.860046 (Recon: 882.275513, KL: 0.031792)\n",
      "[Finetune] Epoch 002 | Train Loss: 541.461742 | Val Loss (Main): 527.050900 (Recon: 516.504528, KL: 0.005273) | Val Loss (Fixed): 883.939392 (Recon: 881.347351, KL: 0.001296)\n",
      "[Finetune] Epoch 003 | Train Loss: 527.006934 | Val Loss (Main): 523.113460 (Recon: 515.816612, KL: 0.003648) | Val Loss (Fixed): 881.616028 (Recon: 879.890991, KL: 0.000863)\n",
      "[Finetune] Epoch 004 | Train Loss: 524.225579 | Val Loss (Main): 520.034935 (Recon: 515.478950, KL: 0.002278) | Val Loss (Fixed): 882.672852 (Recon: 880.018860, KL: 0.001327)\n",
      "[Finetune] Epoch 005 | Train Loss: 522.457092 | Val Loss (Main): 517.044584 (Recon: 515.196540, KL: 0.000924) | Val Loss (Fixed): 882.845581 (Recon: 881.018616, KL: 0.000913)\n",
      "[Finetune] Epoch 006 | Train Loss: 521.684219 | Val Loss (Main): 516.001679 (Recon: 515.123119, KL: 0.000439) | Val Loss (Fixed): 880.127502 (Recon: 879.266724, KL: 0.000430)\n",
      "[Finetune] Epoch 007 | Train Loss: 521.299912 | Val Loss (Main): 515.576083 (Recon: 515.099577, KL: 0.000238) | Val Loss (Fixed): 879.112976 (Recon: 878.748230, KL: 0.000182)\n",
      "[Finetune] Epoch 008 | Train Loss: 521.085873 | Val Loss (Main): 515.310898 (Recon: 514.996749, KL: 0.000157) | Val Loss (Fixed): 881.064453 (Recon: 880.890381, KL: 0.000087)\n",
      "[Finetune] Epoch 009 | Train Loss: 520.947907 | Val Loss (Main): 515.097021 (Recon: 514.898522, KL: 0.000099) | Val Loss (Fixed): 882.082581 (Recon: 882.007263, KL: 0.000038)\n",
      "[Finetune] Epoch 010 | Train Loss: 520.867712 | Val Loss (Main): 515.049954 (Recon: 514.889419, KL: 0.000080) | Val Loss (Fixed): 881.094177 (Recon: 881.038208, KL: 0.000028)\n",
      "Total images loaded: 20538\n",
      "Total images loaded: 346\n",
      "[Finetune] Loaded pretrained weights.\n",
      "[Finetune] Avg power: 12049.319668\n",
      "[Finetune] Start training...\n",
      "[Finetune] Epoch 001 | Train Loss: 4751.561165 | Val Loss (Main): 846.751387 (Recon: 518.839624, KL: 0.163956) | Val Loss (Fixed): 1037.063354 (Recon: 886.806458, KL: 0.075128)\n",
      "[Finetune] Epoch 002 | Train Loss: 568.850855 | Val Loss (Main): 526.932848 (Recon: 516.025510, KL: 0.005454) | Val Loss (Fixed): 879.576538 (Recon: 876.718018, KL: 0.001429)\n",
      "[Finetune] Epoch 003 | Train Loss: 523.916745 | Val Loss (Main): 520.214692 (Recon: 515.433245, KL: 0.002391) | Val Loss (Fixed): 882.607178 (Recon: 878.960022, KL: 0.001824)\n",
      "[Finetune] Epoch 004 | Train Loss: 521.872670 | Val Loss (Main): 518.157954 (Recon: 515.270244, KL: 0.001444) | Val Loss (Fixed): 881.917664 (Recon: 879.425476, KL: 0.001246)\n",
      "[Finetune] Epoch 005 | Train Loss: 521.267012 | Val Loss (Main): 516.781280 (Recon: 515.163288, KL: 0.000809) | Val Loss (Fixed): 880.040649 (Recon: 878.807861, KL: 0.000616)\n",
      "[Finetune] Epoch 006 | Train Loss: 520.985643 | Val Loss (Main): 516.131042 (Recon: 515.051821, KL: 0.000540) | Val Loss (Fixed): 879.839111 (Recon: 879.199341, KL: 0.000320)\n",
      "[Finetune] Epoch 007 | Train Loss: 520.820205 | Val Loss (Main): 515.920214 (Recon: 514.989774, KL: 0.000465) | Val Loss (Fixed): 880.221130 (Recon: 879.707458, KL: 0.000257)\n",
      "[Finetune] Epoch 008 | Train Loss: 520.704694 | Val Loss (Main): 515.716481 (Recon: 514.994181, KL: 0.000361) | Val Loss (Fixed): 880.839722 (Recon: 880.487488, KL: 0.000176)\n",
      "[Finetune] Epoch 009 | Train Loss: 520.641721 | Val Loss (Main): 515.504496 (Recon: 514.948526, KL: 0.000278) | Val Loss (Fixed): 880.685364 (Recon: 880.430664, KL: 0.000127)\n",
      "[Finetune] Epoch 010 | Train Loss: 520.600308 | Val Loss (Main): 515.401500 (Recon: 514.925253, KL: 0.000238) | Val Loss (Fixed): 880.456909 (Recon: 880.234619, KL: 0.000111)\n"
     ]
    }
   ],
   "source": [
    "finetuned_model, device = main_finetune(2048,200,'YourLogNameHere',r'Trained Models/Example VAE Pretrained Model')\n",
    "torch.save(finetuned_model.state_dict(), r\"Trained Models/Example VAE Finetuned Model.pth\")\n",
    "# ===== after finishing one training run =====\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# ===== now you can start next training run =====\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
